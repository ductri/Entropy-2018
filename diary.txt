14:57 17-06-2018
- Download dataset
- Reading final round's description
- Considering IDE: juputer notebook or pycharm --> pycharm, because suggestion is the best way for coding fast.
- Data descriptions: 
+ 3 files, negative, neutral and positive. Each csv file contains 1 column. Each row is a sentence, especially, sentences may be in accented or not. 
+ Each row is just a sentence: need to confirm, that mean, it contains almost 1 dot. And thus, the input will not be too long. Anw, we need a statistic here: sentence long.
+ About accent: A simple idea is remove all accents. More complicated one is predict accents and fill out them to non-accented sentences.
+ According to the problem description in pdf file, there are approximately 150k sentences for all classes, 44.1 MB in size, not so much
+ In addition, organizer also provides list of stop words in vietnamese. 
- Merge 3 files into one
- Export some statistics to have some intuition at data.

16:56 18-06-2018
- Do data description
- TODO: remove too long mention, maybe 1000 as threshold
- Lengh of negative is the longest one, and twice longer than neutral ?
- Data training is quite balance between 3 labels
- do preprocesses
- split data to do validation

00:03 20-06-2018
- Coding skeleton: model_v1.py

- Preprocess
- Build vocabulary + OUT_OF_SCOPE
- Get one-hot index vector
- Build graph, start with a Placeholder in Max Length of a Sentence dimension, followed by transforming matrix, end with 3 nodes

--> Let's build graph first

Step 1: Coding skeleton
Aim to have a runnable version as soon as possible
- Need summary log
- TODO Need data to feed --> preprocess data
- TODO [Maybe next step] Need summary performance on training and evaluation set
11:16 20-06-2018
- Adding summary ops

19:18 20-06-2018
Boost the mood successfully

- Continue to adding summary ops
- Max Ram. Damm! Always remeber to limit resource before run, even dummy test.
- Ok, a simple code is on the board now.
- Let's move to text preprocessing, tedious works :33 
- This is the first try, try it as simple as possible to quickly have a prototype:
+ reuse code from previous round
+ What proportion of sentences without accents ? Not much. TODO consider remove/add accents
+ TODO: it's severe, vietnamese tokenizer doesn't work with non-accent sentences
- ref[1]: https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8
- ref[2]: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
TODO strategies for cutting too much long sentence: cut the tail, cut the head, random cut then get avg
10:44 21-06-2018
- Continue with data manager: loading from files, batching, preprocessing, dumping, ... tired.
19:50 22-06-2018
- TODO replace URL stupid
12:48 22-06-2018
- Haven't run so far, start to be stressed :(
- Straggle with RAM limit. WTH, always end after 470 steps ???
- Adjusting params to fit RAM. Fine. Let it go
- Should get overfitting first to confirm we're having a proper model !!! 
--> Confirmed !

-  create checkpoint, it is a must !!
--> Done
- TODO try this: just pre-train word embedding, then feed to a logistic regression to the a look
- extract parameters with summary name
--> Done
- TODO formally state the pipeline to get a big picture, then consider where on the pipeline should be focused
15:00 23-06-2018
- The skeleton has been quite strong so far
- git tag v1.0
- It was funny, the code would be crash with per_process_gpu_memory_fraction=0.9 but run successfully with per_process_gpu_memory_fraction=0.95
- We are setting the test choosen randomly for each step. It deceases the performance about 80%
17:24 23-06-2018
TODO code predict function 
ref[3] http://fabiorehm.com/blog/2014/09/11/running-gui-apps-with-docker/
TODO read again ref [3]
- THE PIPELINE:
+ split data to training, testing, how about the ratio ?
+ preprocessing, vocabulary size, vocabulary builder
+ build the graph, hyper-parameters: batch size, kernel size, dropout, LC size, number of cnn layers, number of LC layers, vocabulary size. Graph architecture. Word embedding
+ optimizing: learning rate, decay learning rate, momentum, test size
+ measure accuracy, loss ?
+ size of dataset ?

- Coding test the whole testing data
00:16 24-06-2018
- It's seemed that dropout doesn't matter, it just shifts the cross points to the right a bit. Look at "11:34:39|11:40:48". 
- There is nothing absolutely either 1 or 0, it's always a ratio between.
- Dropout may even decreases the performance
TODO code to test on the whole testing dataset. This is the only way to accurately tune hyper-parameters.
- An interesting point: loss is increasing but accurary isn't changing in the test set.
08:51 24-06-2018
- Coding evaluation function
--> Done
- git tag v1.1
- CURRENT STATE: 
+ Model name: 2018-06-24T03:46:38
+ STEP: 8680
  0      0.78     0.72     0.75      9352                                                                                                                	
  1	  0.78      0.78      0.78     11491
  2	  0.82      0.87      0.84		8853

avg / total	  0.79      0.79      0.79     29696
More detail at logs/info_params.log or logs/debug.log

14:58 24-06-2018
- Trying to less capacity model
- TODO summay norm gradient
- Accuracy on test still doesn't increase, approximately 0.79. That proves we are stucking in a local minimum despite of how much capacity the model is. The more capacity the model is, the faster we getting in that stucking point.
- Maybe decreasing capacity helps improving the accuracy., look at 2018-06-24T08:35:22|2018-06-24T08:18:42
- F***, stucking with 0.79
- TODO try to increase training data
20:44 24-06-2018
- Oke, let's go with more training data
--> It may works, but not sure, the testing data size decreases to 10k, acc= ~0.8060
- TODO add dropout in LC layer
--> omg, stupid thing, it almost cannot be learned as the loss doesn't come down. look at 2018-06-24T16:53:35. I guess the reason is about dropout at the last FC, which gets involve the output class.
--> 2018-06-24T17:05:22. model v3 is edited: cut dropout at fc, add relu at last. It seems to work
--> Continue to edit v3: add dropout between 2 FCs with keep_ratio=0.7, experiment name: 2018-06-24T17:17:27. Not much improvement compared to previous
- We really need a revolution
00:27 25-06-2018
- So far, I've been thinking the improment doesn't come from the change of architecture but the increasing of training data
19:37 25:06-2018
- Ok, let's with pretrain embedding. But first, choose the modest model which still get 0.79 accuracy on test.
- Have an idea, padding as the same as out_of_vocab
--> It didn't work
- model v5 is using pretrained word embedding from fastext
13:32 26-06-2018
- Predict samples with model: 2018-06-25T13:48:53
- TODO decrease SENTENCE_LENGTH_MAX=100
--> 75, prone to overfit
- TODO weighted loss
[[1959  760  157]
 [ 311 3009  212]
 [ 142  388 2278]]
19:14 26-06-2018
- Start to code LSTM: model_v6. It took about 1h to code a very simple network :3 :(
Funny bugs: set tf_global_step with int16, so, after a while, it began to decrease to negative
00:20 27-06-2018
Lala, it works, a simple LSTM works, 0.79 was passed. Next obstacle is 0.81
